\section{Evaluation} \label{sec:eva}
We have built the prototype of ROPTerminator. Specifically, the operating system is Linux Ubuntu 12.04 x86 with kernel 3.2.0-29. To avoid recompiling the kernel, we built \name as a loadable kernel module and automatically install it when the system boots up. The kernel module consists of 13K SLOC, in which the adopted disassembly library \emph{diStorm}~\cite{distorm} is around $11K$ SLOC.


\subsection{Experimental Setup}
We evaluated the security and the performance of ROPTerminator prototype using the SPEC CPU2006 benchmark suite~\cite{speccpu2006}, which is processor, memory and compiler stressing, Bonnie++~\cite{bonnie} which is disk I/O stressing and netperf~\cite{netperf} which is network stressing.
The source code of the SPEC CPU2006 benchmark suite is compiled with gcc, g++ version $4.6.3$ with the default Makefile.
We run our experiments on a system with the configurations showed in Table~\ref{tab:config}. %Performance numbers are gathered by averaging 3 runs of each benchmark.

\begin{table}
  \centering
  \begin{tabular}{|l|c|}
  \hline
   Configurations   &   Descriptions\\ \hline
   CPU              &   Intel i5 M540 with 2.53GHZ\\ \hline
   Memory           &   4GB DDR3 1333MHZ\\ \hline
   Network Card     &   Intel 82577LM Gigabit\\ \hline
   Disk             &   320G ATA 7200RPM\\ \hline
  \end{tabular}
  \caption{The configurations of the experiment machine.}\label{tab:config}
\end{table}



\subsection{Security Evaluation}
To verify that our solution is able to defend against all forms of ROP attacks, we performed several tests on one artificial application and a number of realistic applications. %Note that we use ROPEME ~\cite{ropeme} to search gadgets, and use Q to generate payload.

In the first test, we used an artificial program (demonstrated in ROPEME~\cite{ropeme}) that has a simple stack buffer overflow triggered by a long input parameter. We use ROPEME to analyze the program and generate usable gadgets. We manually chain them together to craft a ROP attack that causes the program to start a shell which can facilitate the following attacks.

We then verified our system with a realistic program: a Linux Hex-editer (\emph{htediter}) (2.0.20). The ROP example on the htediter can be found on the web site~\cite{htediter}. An appropriate long input is able to trigger a stack overflow. The template of the ROP payload is available on the website. We simply replace the gadget pointers according to the complied binary in our system. The experiment result shows that \name can detect this attack.

At last, we tested \name against the payloads generated by the general-purpose ROP compiler Q. Q uses semantic program verification techniques to identify the functionality of gadgets and generates a valid ROP payload. Specifically, we generated the payloads for the applications under directory \emph{/bin/} and \emph{/usr/bin/}. Given that there is no known vulnerabilities for such test applications to launch a runtime ROP attack, we had to emulate the environment where the ROP checking algorithm processes each generated payload using the corresponding code region as another input parameter. The experiment results show that the ROP checking algorithm can detect all payloads with no false negative.
%\yueqiang{we may need update the results here with the new payload only with aligned gadgets.}
Specifically, the payload detection algorithm itself can successfully detect 100\% (35/35) payload under directory \emph{/bin}, 93.12\% (203/218) payload under directory \emph{/usr/bin/} and the rest can be handled by alignment filtering algorithm.

\subsection{Performance Evaluation}\label{sec:per}
In this section, we use micro- and macro-benchmark to evaluate the performance of ROPTerminator. And we also evaluate its performance impact to the protected applications and other applications.
%To evaluate the performance of ROPTerminator, we test the micro-benchmark and the macro-benchmark, which not only test the costs of operations introduced by \name, but also evaluate the general effects on the system.
%The micro-benchmark tests the time cost of each extra operation, such as the ROP checking algorithm cost.
%The macro-benchmark aims to evaluate the effects of the protection on the system computation, disk I/O and network I/O.

\subsubsection{Micro-Benchmark}
The micro-benchmark aims to evaluate the time cost of each operation introduced by ROPTerminator. %Some operations (e.g., the ROP checking algorithm) are private, meaning that they are only executed for the protected application, and some operations (e.g., the system call interception) are shared, meaning that they may affect the performance of other applications.
The time cost introduced by system call interception may affect the performance of other application, but other operations (e.g., database installation) are the private cost for the protected application. %All results are list in Table~\ref{tab:syscall} and Table~\ref{tab:step}.

To measure the time cost on the other applications, we install \name but do not execute any protected applications. The system call interception code in the \emph{execve} needs to check the application name, while others only check the process ID (PID). The checking costs are listed in Table~\ref{tab:syscall}. The experiment results show that the time effect on the other applications is quite small.

We then use the \emph{htediter} as an example to measure the time cost of loading BG databases. Note that the time is dependent on the size of the database, e.g., the time cost is 283$\mu s$ for loading database of libc-2.15 ($849.93KB$), and $51.07\mu s$ for database of ld-2.15 ($65.55kB$). The time cost (on average 50.68$\mu s$) is relatively high, but it is only required once in the whole life cycle of the protected application. In addition, the databases of the shared libraries may not be re-loaded if they are loaded by other protected applications. In addition, the database installations may not introduce runtime cost because they are mostly done in the application initialization stage, when the application has not started to run its main function.
%The time cost of the ROP checking is due to the difference of the exit points. In Figure~\ref{fig:checkingcase}, we find that there are $xx\%$ cases returning back before the payload checking.

We also measure each step of the ROP checking algorithm. As shown in Table~\ref{tab:step}, the performance overhead is quite low. Note that the total time cost for a particular checking is not the sum of all these steps. The reason is that some branches (e.g., the payload checking without database) may not be taken in that round. In fact, payload detection (without database) is rarely performed.


\begin{table}
  \centering
  \begin{tabular}{|c|c|}
    \hline
    % aftcer \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    \textbf{Interceptions}         & \textbf{Time} ($\mu s$) \\ \hline
    open                & 0.03 \\ \hline
    close               & 0.04 \\ \hline
    execve              & 0.86 \\ \hline
    mmap                & 0.05 \\ \hline
    mprotect            & 0.03 \\ \hline
    munmap              & 0.04 \\ \hline
    pre-exception       & 0.03 \\ \hline
    post-exception      & 0.01 \\ \hline
  \end{tabular}
  \caption{The micro-benchmark results for one interception. }\label{tab:syscall}
\end{table}


\begin{table}
  \centering
\begin{tabular}{|c|c|c|}\hline
\multicolumn{2}{|c|}{\textbf{Operations}} & \textbf{Time} ($\mu s$)\tabularnewline \hline
\multicolumn{2}{|c|}{Database Installation} & 50.68\tabularnewline \hline
\multicolumn{2}{|c|}{Branch Alignment Checking} & 0.11\tabularnewline \hline
\multicolumn{2}{|c|}{Feedback Checking} & 0.01\tabularnewline \hline
\multirow{2}{*}{Payload Detection} & With Database & 0.93\tabularnewline
\cline{2-3}
            & Without Database & 10.78\tabularnewline \hline
\end{tabular}
  \caption{The time cost in ROP checking and database installation. In most cases, the database installations are performed in the initialization stage of an application.}\label{tab:step}
\end{table}


\subsubsection{Macro-Benchmark}
The macro-benchmark aims to test the general performance impacts of \name on the protected applications themselves and other applications.
We choose three aspects to evaluate: 1) Disk I/O, 2) Network I/O, and 3) Computation. Generally, the impact of \name on the whole platform is small, and there is no significant overhead on the above three aspects.

\textbf{Disk I/O Performance Evaluation.}~
We choose the benchmark tool Bonnie++ (version 1.96) to evaluate the disk I/O. The tool attempt to sequentially read/write data from/to a particular file in different ways. The read/write granularity is from a char to a block (i.e., 8192 Bytes). Furthermore, we also test the time cost of the random seeking. Figure~\ref{fig:disk} shows the disk I/O measurement results, which indicate the performance overhead on the disk I/O is quite low.
%the command is \emph{bonnie++ -d /tmp/foo}
%\yueqiang{the performance is unstable!!!}

\begin{figure}[!ht]
 \centering
\includegraphics[width=0.99\columnwidth]{"image/disk"}
\caption{\textbf{The disk performance results.} }\label{fig:disk}
\end{figure}

\textbf{Network I/O Performance Evaluation.}~
We choose the benchmark tool Netperf (version 2.5.0) to evaluate the network I/O. In our experiment, two machines are directly connected through a network cable. The tool on one machine attempts to send/receive network packets to/from a socket built on another machine. We test two popular protocols (i.e., TCP and UDP) with the same packet size. Figure~\ref{fig:network} shows the experiment results, which indicate the performance overhead on the network I/O is quite low. That is because  we do \emph{not} intercept the system calls (e.g., send) related to network sending or receiving. Thus, the test results are the same..
%the command for TCP connection is \emph{netperf -l 120 -H 192.168.1.102 -t TCP\_STREAM -- -m 16384}

\begin{figure}[!ht]
 \centering
\includegraphics[width=0.9\columnwidth]{"image/network"}
\caption{\textbf{The network performance results.} }\label{fig:network}
\end{figure}

\textbf{SPEC CPU Benchmark.}~
We choose the benchmark tool SPEC CPU2006 benchmark suite to evaluate the computation performance. Specifically, we first execute the benchmark tools with no protection, and then we execute them under the protection of ROPTerminator. The results are illustrated in Figure~\ref{fig:specint}), which shows that ROPTerminator only introduce 2\% performance loss on average.
%\yueqiang{I think we should analyze the cost of xalan.}

\begin{figure*}[!ht]
 \centering
\includegraphics[width=1.9\columnwidth]{"image/specint"}
\caption{\textbf{The SPEC INT2006 Benchmark Results.} The optimized algorithm generally reduces the performance overhead compared to the basic one. }\label{fig:specint}
\end{figure*}
