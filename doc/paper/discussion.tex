\section{Discussion}\label{sec:dis}
%\zhi{Macrobenchmark results of netperf do not reveal IOTLB's impacts on the DMA transactions.}

\subsection{IOTLB Misses}
According to IOMMU specification~\cite{intelvt, amdvt}, a typical IOMMU is able to provide three types of IOTLB invalidation schemes, i.e., global invalidation, domain-selective invalidation, page-selective invalidation, which differ in granularity.
Specifically, the global invalidation will always invalidate all IOTLB entries as a whole.
The domain-selective invalidation only invalidates the selected VM domain's IOTLBs, whose performance is a little better than the global invalidation.
The page-selective invalidation that only invalidate the corresponding IOTLB could achieve the best performance, comparing with the previous two schemes.

If the IOMMU is configured to do the global or domain-selective invalidation, an IOTLB flush produced by the DMA validations will invalidate all related IOTLB entries, inevitably resulting in IOTLB misses for all DMA address translations. However, if the IOMMU is working with the page-selective invalidation scheme, one IOTLB flush will only invalidate one IOTLB entry, which may not be used immediately. It means the current DMA request may not be affected, but its effects may result in the future DMA transfer.
%Currently, we do not evaluate the relationship between the IOTLB flushes and IOTLB misses in the page selective invalidation scheme, and we plan to put it into the future work.

\subsection{Automatically Shrinking \name Cache}
In the previous design, the \cache cannot automatically shrink itself.
As talked before, when to free pages in cache relies on both the memory size of the cache and the proportion between the cache size and page-table size. Default thresholds of the two factors are determined by a particular setting created by a specific stress tool. Therefore, both thresholds may not work in other settings. If the thresholds are not set appropriately, freeing pages will occur often, thus causing IOTLB flush. This is why we provide an interface for users to manually modify the default thresholds. Nevertheless, the optimum tradeoff between cache size and IOTLB-flush is not easy to reach only by the simple interface and we plan to propose a self-adaption algorithm in the future work. Basically, the algorithm has the properties as follows.
\begin{enumerate}
\item Memory usage of the cache is under control.
\item Frequency of IOTLB-flush will drop to a lowest level.
\item The frequency will reach the level as soon as possible.
\end{enumerate}

Firstly, an upper limit to the cache size is set based on the memory usage of a target application. Under the limit, the algorithm manages to eliminate IOTLB-flush with a fewest cache usage. By scanning the the frequency of IOTLB-flush periodically, the algorithm will appropriately adjust the cache size based on both the memory pages in cache and in page tables residing in the target application.


%When users dynamically enable the cache for the application, the algorithm is also invoked to record the memory usage by scanning and then determine the proportion between them. If IOTLB-flush occurs during the period, increase the cache size. If the cache exceeds a particular percentage of , it is not supposed to be any higher even if IOTLB still flushes.

\subsection{\name Cache on the Bare-metal OS}
We believe the design of the \cache could benefit the page table allocations and deallocation on the bare-metal OSes that directly work on hardware.
The usage pattern of the page table pages in paravirtual environment is similar if not the same to the one on the bare-metal OS.
Based on this feature, the deallocated page table pages are likely to be used in the near future by a newly created processes.
By caching the deallocated page-table pages, the \cache could quickly response to the following allocation requests, without needing to invoke the system allocators every time.
In the future, we plan to port the \cache onto a bare-metal OS, such as Linux, and fully evaluate its benefits.
