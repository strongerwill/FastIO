\section{Introduction} \label{sec:intro}
In the paravirtualization~\cite{XEN-SOSP03,denali-paravirtualization}, the kernel of each guest Virtual Machine (VM) and the hypervisor share the same virtual space including many security-critical data structures, like page tables and Global Descriptor Table (GDT). All these data structures are managed by the guest VM, but their updates are intercepted and validated by the hypervisor~\cite{XEN-SOSP03} with the purpose of preventing malicious accesses from the guest software.
However, the paravirtualization technology itself is not armed with efficient protection to prevent DMA attacks~\cite{disaggregation}.
To fix this gap, the hypervisor has to resort to the I/O virtualization (AMD-Vi~\cite{amdvt} or Intel VT-d~\cite{intelvt}) technology, which introduces a new Input/Output Memory Management Unit (IOMMU) to restrict DMA accesses on the physical memory addresses occupied by the hypervisor and the shared security-critical data structures.
Leveraging the combination of the paravirtualization and I/O virtualization, the hypervisor prevents all malicious accesses from processor and hardware peripheral devices.

%Highlight the problem, and emphasise the importance.
\textbf{Problem.} Although the combination of the paravirtualization and I/O virtualization brings strong security protection, we surprisingly find out that it slows down the speed of DMA address translation due to the additional I/O translation look-aside buffer (IOTLB) flushes.
Specifically, the guest page tables should be protected from every DMA access.
Thus, when a writable page\footnote{A \emph{writable page} is the page in the guest VM, allowing the guest VM to freely read and write according to its own purpose.} turns into a page-table page, the hypervisor has to update the IOMMU page table and flush an IOTLB entry to prevent DMA from accessing it.
The IOTLB flush is necessary for the sake of the security of the hypervisor, but it inevitably increases the miss rate of IOTLB, and consequently reduces I/O performance, especially for the high-speed devices.
Similarly, when a page-table page is turned to a writable page, the hyperisor needs to return the ownership to the guest VM.
As a result, the hypervisor has to update the IOMMU page table and flush IOTLBs for allowing the guest VM to freely access the returned writable page.
In addition, these page-type changes between page-table pages and writable pages are \emph{often} triggered during the whole life cycle of a running system.
As a consequence, the IOTLB flushing events are \emph{frequently} triggered, which inevitably lower the speed of the DMA address translation and further badly affect I/O performance of all peripheral devices.

%existing work

%introduce our approach
This new dependency issue between guest page tables and the DMA transferring urges us to reshape the design of the hypervisor and the guest operating system to minimize the effects on the I/O performance.
In response to this problem, we aim to improve the guest kernel and the hypervisor to reduce the flush rate of IOTLB, and therefore ameliorate its impacts on the I/O performance of all peripheral devices.

Our approach rests on the observation that the high IOTLB flush rates experienced by a guest VM
are due to the page-type changes generated by the numerous creations and destructions of page tables.
We cannot reduce the number of the creation/destruction of page table, but reducing the number of page-type change is possible.
Inspired by this observation, we propose the page-table cache, which queues the released/freed page-table page in the hope
it will be reused (popped out the cache) in the creation of page table in the near future.
During this process, the destruction and the creation of the page table will not introduce page type change, and therefore there will be no IOTLB flush.
Besides minimizing the IOTLB flush rate, the page-table cache also accelerates the creation and destruction of page table.
The reason is that the kernel does not need to ask for pages from enormous and costly memory management subsystem, instead it could quickly get pages from the page-table cache with very small cost.
In order to allow the kernel memory management subsystem to use the pages cached in the page-table cache, we add an interface for the page-table cache to free pages.

We implement the page-table cache with small modifications of hypervisor Xen version 4.2.1 (xxx SLoC) and Linux kernel version 3.2.0 (xxx SLoC), and evaluate the I/O performance in micro and macro ways.
The micro experiment results indicate that the new page-table cache is able to effectively reduce the miss rate of IOTLB (from xxx to zero) with \emph{less CPU usage}, even when the page tables are frequently updated.
The macro benchmarks shows that the I/O devices always produce better (or the same) performance, even when the system frequently generates many temporal processes.


In particular, we make the following contributions:
\begin{enumerate}
\item We are the first, to the best of our knowledge, to identify the dependency issue between guest page table and DMA transferring.
\item We proposed a novel approach - page table cache, to improve I/O performance of all peripheral devices by minimizing the flush rate of IOTLB, without sacrificing the system security.
\item We implemented a prototype of the page table cache and evaluated the performance in both micro and macro ways.
\end{enumerate}

The rest of the paper is structured as follows: In Section~\ref{sec:preli} and Section~\ref{sec:prob}, we briefly describe the background knowledge, and highlight our goal and the thread model. In Section~\ref{sec:rationale} we discuss the design rationale. Then we describe the system overview and implementation in Section~\ref{sec:overview} and Section~\ref{sec:implement}. In Section~\ref{sec:eva}, we evaluate the security and performance of the system, and discuss several attacks and possible extension in Section~\ref{sec:dis}. At last, we discuss the related work in Section~\ref{sec:related}, and conclude the whole paper in Section~\ref{sec:con}.

