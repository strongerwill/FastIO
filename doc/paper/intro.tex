\section{Introduction} \label{sec:intro}
In the paravirtualization~\cite{XEN-SOSP03,denali-paravirtualization}, the kernel of each guest Virtual Machine (VM) and the hypervisor share the same virtual space including many security-critical data structures, like page tables and Global Descriptor Table (GDT). All these data structures are manageed by the guest VM, but their updates are intercepted and validated by the hypervisor~\cite{XEN-SOSP03} with the purporse of preventing malicious accesses from the guest software.
However, the paravirtualization technology itself is not armed with efficient protection to prevent DMA attacks~\cite{disaggregation}.
To fix this gap, the hypervisor has to resort to the I/O virtualization (AMD-Vi~\cite{amdvt} or Intel VT-d~\cite{intelvt}) technology, which introduces a new Input/Output Memory Management Unit (IOMMU) to restrict DMA accesses on the physical memory addresses occupied by the hypervisor and the shared security-critical data structures. 
Leveraging the combination of the paravirtualization and I/O virtualization, the hypervisor prevent all malicious accesses from processor and hardware perpherial devices.

%Highlight the problem, and emphasise the importance.
\textbf{Problem.} Although the combination of the paravirtualization and I/O virtualization brings strong security protection, but we surperisingly find out that it leads to the reduction of the DMA address translation due to the additional IOTLB flushes. 
Specifically, the paravirtualized hypervisor needs to protect all guest page tables 
by setting them read-only and preventing all DMA accesses.  
In addition, the guest page tables are not static/constant data structures, 
instead they are created and destroied by the guest kernel.
Thus, the hypervisor needs to trace the page-type changes to update the IOMMU page table and flush IOTLBs.
For instance, when a page is changed from a writable page\footnote{The \emph{writable pages} are allowed to be read and written by the guest VM.}  
to a page-table page, the hypervisor has to update the IOMMU page table and flush IOTLB to prevent any DMA access on it.
These flushes are necessary for the sake of the security of the hypervisor, but it inevitably increases the miss rate of IOTLB, and consequently reduces I/O performance, especially for the high-speed devices. 
In addition, these updates and changes are often triggered during the whole life cycle of a running system.
As a consequence, the IOTLB flushing events are frequently triggered, which inevitably lower the speed of the DMA address translation and the I/O performance of all peripheral devices.

%existing work

%introduce our approach
This new dependency issue between guest page tables and the DMA transferring urges us to reshape the design of the guest operating system and the hypervisor to minimize the effects on the I/O performance. 
To response to this problem, we aim to improve the guest kernel and the hypervisor to reduce the flush rate of IOTLB, and therefore ameliorate its effects on the I/O performance of all peripheral devices.

Our approach rests on the observation that the high IOTLB flush rates experienced by a guest VM 
are due to the page-type changes generated by the numerous creations and destructions of page tables. 
We cannot reduce the number of the creation/destruction of page table, but reducing the number of page-type change is possible.
Inspired by this observation, we propose the page-table cache, which queues the released/freed page-table page in the hope 
it will be reused (poped out the cache) in the creation of page table in the near future. 
During this process, the destruction and the creation of the page table will not introduce page type change, and therefore there will be no IOTLB flush happens.
Besides minimizing the IOTLB flush rate, the page-table cache also accelarates the creation and destruction of page table.
The reason is that the kernel does not need to ask for pages from enormous and costly memory management subsystem, instead it could quickly get pages from the page-table cache with very small cost. 
In order to allow the kernel memory management subsystem to use the pages cached in the page-table cache, we add an interface for the page-table cache to free pages.

We implement the page-table cache with small modifications of hypervisor Xen (xxx SLoC)  and Linux kernel version 3.2.0 (xxx SLoC), and evaluate the I/O performance in micro and macro ways. 
The micro experiment results indicate that the new page-table cache is able to effectively reduce the miss rate of IOTLB (from xxx to zero) with \emph{less CPU usage}, even when the page tables are frequently updated. 
The macro benchmarks shows that the I/O devices always produce better (or the same) performance, even when the system frequently generate many temporal processes.


In particular, we make the following contributions:
\begin{enumerate}
\item We are the first, to the best of our knowledge, to identify the dependency issue between guest page table and DMA transferring. 
\item We propseed a novel approach - page table cache, to improve I/O performance of all peripheral devices by minimizing the flush rate of IOTLB, without sacrificing the system security.
\item We implemented a protype of the page table cache and evaluated the performance in both micro and macro ways.
\end{enumerate}

The rest of the paper is structured as follows: In Section~\ref{sec:preli} and Section~\ref{sec:prob}, we briefly describe the background knowledge, and highlight our goal and the thread model. In Section~\ref{sec:rationale} we discuss the design rationale. Then we describe the system overview and implementation in Section~\ref{sec:overview} and Section~\ref{sec:implement}. In Section~\ref{sec:eva}, we evaluate the security and performance of the system, and discuss several attacks and possible extension in Section~\ref{sec:dis}. At last, we discuss the related work in Section~\ref{sec:related}, and conclude the whole paper in Section~\ref{sec:con}. 

