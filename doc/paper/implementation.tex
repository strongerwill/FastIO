\section{Implementation} \label{sec:implementation}
\yueqiang {
implementations:
1) hooks for on-demand enable
2) locks for algorithm
3) data structures in algorithm
4) threshold selection for cache free? or this discussed in design
5) }

\subsection{Hypervisor Space}
In a 32-bit field related to page type information, \name was planning to use a redundant bit to represent the cache flag in order to be compatible with its original implementation. However, each bit in the upper 9-bit of the field has its specific use and the lower 23-bit serves as the reference count of current page type, representing at most ($2^23-1$) reference counts of the page type. \name enforces a stricter rule to prevent count overflow, as cache flag occupies the top bit of the 23-bit. Also, \name adds several SLOC in the function \_\_get\_page\_type() to check if a machine has the flag bit. Besides, if each level of cache pool in the guest OS submits a list of cached pages that are going to be freed, respectively \name implements a hypercalls to clear the flag bit of all machine pages, create corresponding entries in the I/O page tables and flush IOTLB.   

\subsection{Guest OS Space}

In the PV setting, guest OS is required to work in PAE (i.e., Physical Address Extension) mode. As a result, \name needs to build up three levels of cache pools. For the top two levels used for PGD (i.e., Page Global Directory) and PMD (i.e., Page Middle Directory), \name maintains a structured single-linked list caching the linear address of each page for each level of cache pool. As for the PT (i.e., Page Table) located in the High Memory, \name maintains a simliar list but caching the structured page info of pages, since the mapping between linear addresses and physical addresses is not stable every time the pages are allocated/deallocated by PT functions.  

Specifically, \name defines new cache allocating and freeing functions for every level of page table, hooks page-table related functions using the cache functions, and builds up all levels of cache pools. The data structure used to manage every level of pool is  and \name always fetches pages from or frees them into the top of the list, trying to perform well. 

If OS asks more pages (e.g., many processes creations) than cache pool supplies, \name will request pages from the buddy system. What if the memory sapce that caches take up is too high? it is necessary to free pages from the pool to the buddy system. When to free pages in pool depends on $1$) a proportion between pages in use and in pool, and $2$) a total number of pages in use and in pool. If both factors exceed specific valuess, respectively, \name executes freeing operation. The values are obtained through experiments (see details in section~\ref{sec:eva}) and the page numbers to free are stated in the equation below: $\Delta$num\_to\_free $=$ num\_in\_pool $-$ num\_in\_use. \name also provides an interface for users to modify the values of two factors so as to adjust the size of cache pool. As the safe flag must be cleared before pages are freed, \name defines a new hypercall to ask Xen for zeroing the flag bit and removing DMA protection. Also note that \name not only supports a preliminary enabled cache algorithm with system booting up, but also develops a kernel loadable module for users to activate it dynamically especially when OS creates processes frequently. Utilizing this feature, users can improve system performance in an on-demand way.  

